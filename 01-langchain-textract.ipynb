{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb56bc46-004a-48de-9161-80d0b9ac5678",
   "metadata": {},
   "source": [
    "# Amazon Textract LangChain Document Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f91784c-8ca6-4b2e-921a-35cc2630df90",
   "metadata": {},
   "source": [
    "Amazon Textractâ€™s new Layout feature introduces efficiencies in general purpose and generative AI document processing tasks: [Blog](https://aws.amazon.com/blogs/machine-learning/amazon-textracts-new-layout-feature-introduces-efficiencies-in-general-purpose-and-generative-ai-document-processing-tasks/)\n",
    "\n",
    "The Textract team has also provided a document loader for LangChain which uses Amazon Textract layout feature, see the [AmazonTextractPDFParser](https://aws.amazon.com/blogs/machine-learning/amazon-textracts-new-layout-feature-introduces-efficiencies-in-general-purpose-and-generative-ai-document-processing-tasks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "748d26ef-5126-4070-ad69-03d15a6b3462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pypdf Pillow amazon-textract-caller==0.2.4 amazon-textract-textractor==1.8.3 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ac35aad-06ba-4dc5-a30f-9e0399ec0872",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Default sagemaker bucket: sagemaker-us-west-2-557690596620\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Get the SageMaker session and default bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(f\"Default sagemaker bucket: {default_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b5f3e27-b39e-43ea-94b8-36e3a1b308c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#copy data folder with sample pdf files to S3\"\n",
    "!aws s3 sync ./data s3://{default_bucket}/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53f6f3cc-8e4a-462b-a234-13d813a05738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import AmazonTextractPDFLoader\n",
    "from textractor.data.text_linearization_config import TextLinearizationConfig\n",
    "\n",
    "document = \"data/paper-llm-training-sample.pdf\"\n",
    "document_s3_url = f\"s3://{default_bucket}/{document}\"\n",
    "\n",
    "linerization_config = TextLinearizationConfig(hide_header_layout=False,hide_footer_layout=False, hide_figure_layout=False, table_add_title_as_caption=True, table_linearization_format=\"markdown\")\n",
    "\n",
    "loader = AmazonTextractPDFLoader(document_s3_url, textract_features=[\"TABLES\", \"LAYOUT\", \"FORMS\"], linearization_config=linerization_config)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9515f5f4-9793-4444-a613-b7517cb7c666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"600\"\n",
       "            src=\"data/paper-llm-training-sample.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f85fb6defe0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(document, width=400, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e76b6737-671c-4f8b-b6da-eddf64fbb7ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Haozheng Fan, Hao Zhou, Guangtai Huang, Parameswaran Raman, Xinwei Fu, Gaurav Gupta, Dhananjay Ram, Yida Wang, and Jun Huan \n",
      "\n",
      "reduce training latency with asynchronous execution. This overlaps some executions of accelerators and host (CPU). \n",
      "\n",
      "4 TRAINING PROCESS \n",
      "\n",
      "4.1 Training Curves \n",
      "\n",
      "During the training process, we monitor the training loss, as well as l2 norm of gradients and l2 norm of parameters for debugging training stability. Figure 1a shows the training loss over global batches, reduced over all data parallel ranks. The training loss decreases fast for the initial ~250B tokens, and enters a log-linear decrease afterwards. Similar trends are observed in other LLM training [18,51,52]. \n",
      "\n",
      "In Figure 1b, we show the gradient l2 norm during the training. Overall, we see that the gradient norm is stable across the training journey without divergence. Note that gradient spikes are immi- nent in LLM pre-training when using layer-normalization, or even RMSNorm [50], and some times due to overflow in low-precision, such as 16-bit floats. We show an assuring trend in Figure 1b even with using 16-bit floats such as BF16. Furthermore, we clip the gradient-norm to 1.0 (see Section 3.2) which also allows for magni- tude stabilization. Note that sustained spikes in the gradient norm leads to training divergence due to improper weight updates, even after gradient normalization through clipping. In Figure 2, we show that the gradient spikes often last for a single step, and did not lead to training divergence. Specifically, we first track a running average (r) of gradient norm over a window of 20 steps to smooth out the natural fluctuations due to batching. We define occurrence of a gra- dient spike when the current gradient norm is higher than r + 0.1. Next, we track the number of steps for gradient norm returning to less than ++0.1. Over 86%, the spike deviates from running average for only a single step. \n",
      "\n",
      "Finally, we show the parameter l2 norm in Figure 1c. During first ~250B tokens, the parameter norm increases consistently. This phase also coincides with the fast decreasing phase of training loss where model parameters converge from random initialization to a structured distribution. After that, the parameter norm consistently decrease since AdamW applies weight decay for regularization [35]. \n",
      "\n",
      "4.2 Hardware and System Failures \n",
      "\n",
      "The pre-training process can be interrupted due to hardware fail- ures, communication timeouts, etc [6]. We monitor the node active time and frequency of training interruptions. As listed in Table 1, the overall system uptime of HLAT pre-training is 98.81%. With automatic fault recovery mechanism in NDTL (see Section 2), the training quickly recovers from failures automatically without wast- ing much time. For comparison, we also listed another experimental training run (over 600 billion tokens) without automatic fault re- covery, and we observed an average of 20% lower system up time. \n",
      "\n",
      "Table 1: Percentage of system uptime with vs without auto- matic fault recovery. \n",
      "\n",
      "\n",
      "\n",
      "| With Fault Recovery    | Without Fault Recovery    |\n",
      "|------------------------|---------------------------|\n",
      "| 98.81%                 | 77.83%                    |\n",
      "\n",
      "\n",
      "\n",
      "Table 2: Comparison of model architectures and number of training tokens. \n",
      "\n",
      "\n",
      "\n",
      "| Model Name    | Size    |   Sequence length  | Tokens    |\n",
      "|---------------|---------|--------------------|-----------|\n",
      "| HLAT          | 7B      |              4096  | 1.8T      |\n",
      "| LLaMA-1       | 7B      |              2048  | 1T        |\n",
      "| LLaMA-2       | 7B      |              4096  | 2T        |\n",
      "| OpenLLaMA-1   | 7B      |              2048  | 1T        |\n",
      "| OpenLLaMA-2   | 7B      |              2048  | 1T        |\n",
      "\n",
      "\n",
      "\n",
      "4.3 Training Instability \n",
      "\n",
      "We describe a few changes we made before and during the training process for convergence and training stability. \n",
      "\n",
      "Initialization: We use a scaled initialization strategy for initial- lizing model parameters. Specifically, the initial standard deviation of output layers in attention blocks and MLP layers are scaled by 1/V2l where l is the layer index. Similar as discussed in [50], we found better numerical stability and convergence with smaller initial variance on deeper layers. In addition, all parameters are initialized on CPU and offloaded to TRAINIUM. \n",
      "\n",
      "Normalization: We used tensor parallelism to shard the model parameter matrices except normalization layers. The normalization layer weights, however, are slightly different across TP ranks due to stochastic rounding. Empirically, we found the differences are small, and RMSNorm weights values are all close to 1. OLMo [19] used non-parametric layer norm, which is equivalent as all weights equals 1. Both HLAT and OLMo show similar quality as LLaMA, despite of differences in normalization weights. \n",
      "\n",
      "Gradient Synchronization: Since Neuron SDK uses a different underlying collective library as other accelerators. It needs careful attention while writing distributed training library, such as NDTL. Mis-referencing APIs may cause training instability. \n",
      "\n",
      "Neuron Persistent Cache on Local Worker: In HLAT train- ing, all instances share a same file system using Amazon FSx8 for storing data, checkpoints, logs, etc. However, we found that stor- ing Neuron Persistent Cache9 on FSx may cause communication bottleneck because those cached graphs are frequently accessed by all TRAINIUM devices in the cluster. Such bottleneck may lead to communication timeout and affects training stability. Therefore, we instead store Neuron Persistent Caches in file system of each local worker. \n",
      "\n",
      "5 EVALUATION \n",
      "\n",
      " Baselines: include LLaMA-1 7B [51], LLaMA-2 7B [52], We evaluate HLAT against several open-source bench- mark models. Since HLAT structure is similar as LLaMA model, we OpenLLaMA-1 7B and OpenLLaMA-2 7B [18]. The model architecture and composition of the training data of the models being compared are listed in Table 2. OpenLLaMA-1 model is trained on RedPajama [14] dataset. OpenLLaMA-2 model shares same structure as OpenLLaMA-1, but is trained on a different data mixture which includes data from Falcon-RefinedWeb [40], StarCoder [32], and RedPajama [14]. \n",
      "\n",
      "8https://aws.amazon.com/fsx/\n",
      " https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-\n",
      " features/neuron-caching.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the linearized layout-aware text of one page\n",
    "print(documents[3].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e533b1-ec89-4e14-9810-815031c73b34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
