{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb56bc46-004a-48de-9161-80d0b9ac5678",
   "metadata": {},
   "source": [
    "# Amazon Textract LangChain document loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f91784c-8ca6-4b2e-921a-35cc2630df90",
   "metadata": {},
   "source": [
    "Amazon Textract’s new Layout feature introduces efficiencies in general purpose and generative AI document processing tasks: [Blog](https://aws.amazon.com/blogs/machine-learning/amazon-textracts-new-layout-feature-introduces-efficiencies-in-general-purpose-and-generative-ai-document-processing-tasks/)\n",
    "\n",
    "The Textract team has also provided a document loader for LangChain which uses Amazon Textract layout feature, see the [AmazonTextractPDFParser](https://aws.amazon.com/blogs/machine-learning/amazon-textracts-new-layout-feature-introduces-efficiencies-in-general-purpose-and-generative-ai-document-processing-tasks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "748d26ef-5126-4070-ad69-03d15a6b3462",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf Pillow amazon-textract-caller amazon-textract-textractor --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ac35aad-06ba-4dc5-a30f-9e0399ec0872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Default sagemaker bucket: sagemaker-us-east-1-626723862963\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Get the SageMaker session and default bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(f\"Default sagemaker bucket: {default_bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b5f3e27-b39e-43ea-94b8-36e3a1b308c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy data folder with sample pdf files to S3\"\n",
    "!aws s3 sync ./data s3://{default_bucket}/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53f6f3cc-8e4a-462b-a234-13d813a05738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import AmazonTextractPDFLoader\n",
    "from textractor.data.text_linearization_config import TextLinearizationConfig\n",
    "\n",
    "document = \"data/2023.01.11.523679v1.full.pdf\"\n",
    "document_s3_url = f\"s3://{default_bucket}/{document}\"\n",
    "\n",
    "linerization_config = TextLinearizationConfig(hide_header_layout=False,hide_footer_layout=False, hide_figure_layout=False)\n",
    "\n",
    "loader = AmazonTextractPDFLoader(document_s3_url, textract_features=[\"TABLES\", \"LAYOUT\", \"FORMS\"], linearization_config=linerization_config)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e76b6737-671c-4f8b-b6da-eddf64fbb7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bioRxiv preprint doi: https://doi.org/10.1101/2023.01.11.523679 this version posted January 15, 2023. The copyright holder for this preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made available under aCC-BY-ND 4.0 International license. \n",
      "\n",
      " The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics \n",
      "\n",
      "Hugo Dalla-Torre¹, Liam Gonzalez¹, Javier Mendoza Revilla¹ Nicolas Lopez Carranza ¹, Adam Henryk Grzywaczewski Francesco Oteri1, Christian Dallago 2 3 Evan Trop1, Hassan Sirelkhatim ², Guillaume Richard1 Marcin Skwark1 Karim Beguir1 Marie Lopez*t 1 , Thomas Pierrot*t 1 \n",
      "\n",
      "\n",
      "1InstaDeep 2Nvidia TUM \n",
      "\n",
      "\n",
      "\n",
      "Abstract \n",
      "\n",
      "\n",
      "Closing the gap between measurable genetic information and observable traits is a longstanding challenge in genomics. Yet, the prediction of molecular phenotypes from DNA sequences alone remains limited and inaccurate, often driven by the scarcity of annotated data and the inability to transfer learnings between prediction tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named the Nucleotide Transformer, integrating information from 3,202 diverse human genomes, as well as 850 genomes from a wide range of species, including model and non-model organisms. These transformer models yield transferable, context-specific representations of nucleotide sequences, which allow for accurate molecular phenotype prediction even in low-data settings. We show that the representations alone match or outperform specialized methods on 11 of 18 prediction tasks, and up to 15 after fine-tuning. Despite no supervision, the transformer models learnt to focus attention on key genomic elements, including those that regulate gene expression, such as enhancers. Lastly, we demonstrate that utilizing model representations alone can improve the prioritization of functional genetic variants. The training and application of foundational models in genomics explored in this study provide a widely applicable stepping stone to bridge the gap of accurate molecular phenotype prediction from DNA sequence alone. \n",
      "\n",
      "Introduction \n",
      "\n",
      "Foundation models in artificial intelligence (AI) refer to large models incorporating millions of param- eters and trained on vast amounts of data, which can be adapted for an array of predictive purposes. These models have deeply transformed the AI field with notable examples in natural language includ- ing the so-called language models (LMs) and GPT-3 [2]. LMs have gained considerable BERT [1] popularity over the past years, owing to their capacity to be trained on unlabeled data to create general-purpose representations that can solve downstream tasks. One way they achieve a general understanding of language is by solving billions of cloze tests in which, given a sentence with some blanked-out words, they are rewarded by suggesting the correct word to fill the gap. This approach is referred to as masked language modelling [1]. Early examples of foundation models leveraging this objective in biology were trained on protein sequences by tasking LMs to uncover masked amino acids in large protein sequence datasets [3, 4, 5]. Trained protein LMs applied to downstream tasks, in what is called transfer learning, showed an aptitude to compete and even surpass previous methods for protein structure [3, 4] and function predictions [6, 7], even in data scarce regiments [8]. \n",
      "\n",
      "Beyond protein sequences, the dependency patterns embedded in DNA sequences are fundamental to the understanding of genomic processes, from the characterization of regulatory regions to the impact of individual variants in their haplotypic context. Along this line of work, specialized deep learning (DL) models were trained to identify meaningful patterns of DNA, for example, to predict gene expres- sion from DNA sequences alone [9, 10, 11, 12, 13], with recent work combining convolutional neural \n",
      "\n",
      "*Equal Supervision\n",
      "\n",
      "Corresponding authors: t.pierrot@instadeep.com & m.lopez@instadeep.com \n",
      "\n",
      "\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e533b1-ec89-4e14-9810-815031c73b34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
